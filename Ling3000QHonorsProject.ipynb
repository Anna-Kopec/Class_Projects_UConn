{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Honors Project - LING 3000Q\n",
        "Prof. Stefan Kaufmann\n",
        "\n",
        "Anna Kopec\n",
        "\n",
        "4/21/2025"
      ],
      "metadata": {
        "id": "Kqfbi6P04NWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Overview:\n",
        "I completed all given excercises from ch. 18 in the Jurafsky and Martin textbook (to the best of my ability)\n",
        "- **convert_to_cnf(grammar):** convert arbitrary grammars into CNF format\n",
        "- **CKY(words, grammar):** given a sentence and corresponding grammar, parse all possible linguistic tree structures\n",
        "- **PARSEVAL**: test CKY parser using data from Penn Treebank dataset and implement standard parse tree evaluator using PARSEVAL library\n"
      ],
      "metadata": {
        "id": "R7VmsM_QMjAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import CFG, Nonterminal, Production, Tree, induce_pcfg\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import re\n",
        "!pip install svgling\n",
        "import svgling"
      ],
      "metadata": {
        "id": "x2ClcZ3ZegmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fafbaea-5769-4e70-e94a-887d201feaec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgling\n",
            "  Downloading svgling-0.5.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Downloading svgling-0.5.0-py3-none-any.whl (31 kB)\n",
            "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.5.0 svgwrite-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18.1\n",
        "Implement the algorithm to convert arbitrary context-free grammars to CNF. Apply your program to the L1 grammar"
      ],
      "metadata": {
        "id": "K-iuBKDI4CKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copied grammar from textbook and added some addional productions for a larger grammar\n",
        "L1_grammar = \"\"\"\n",
        "S -> NP VP\n",
        "S -> Aux NP VP\n",
        "NP -> Pronoun\n",
        "NP -> Proper-Noun\n",
        "NP -> Det Nominal\n",
        "Nominal -> Noun\n",
        "Nominal -> Nominal Noun\n",
        "Nominal -> Nominal PP\n",
        "VP -> Verb\n",
        "VP -> Verb NP\n",
        "VP -> Verb PP\n",
        "VP -> Verb NP PP\n",
        "PP -> Preposition NP\n",
        "\n",
        "Aux -> 'will' | 'can' | 'should'\n",
        "Verb -> 'book' | 'include' | 'prefer'\n",
        "Pronoun -> 'I' | 'she' | 'me'\n",
        "Proper-Noun -> 'United' | 'Houston'\n",
        "Nominal -> 'book' | 'include' | 'prefer'\n",
        "Noun -> 'book' | 'trip' | 'hotel'\n",
        "Det -> 'the' | 'a'\n",
        "Preposition -> 'in' | 'on' | 'of'\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6yP63eo9g8kF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_CNF(grammar):\n",
        "    '''This method converts an arbitrary grammar into Chomsky Normal Form by making sure that:\n",
        "    -Each rule is binary, and if not, implement dummy non-terminals to make it binary\n",
        "    -Terminals and non-terminals are not mixed together in rules\n",
        "    Although usually CNF means no unit productions (NT->NT), I altered my parser algorithm to include unit productions.\n",
        "    *method assumes that grammars don't have empty productions'''\n",
        "\n",
        "    # store new CNF grammar here\n",
        "    L2 = []\n",
        "\n",
        "    # padding for dummy non-terminals\n",
        "    padding = 'X'\n",
        "    rule_count = 1\n",
        "\n",
        "    # Build terminal-to-nonterminal mapping (for getting rid of mixed terminals/non-terminals later)\n",
        "    terminal_map = defaultdict(list)\n",
        "    for rule in grammar.productions():\n",
        "        rhs = rule.rhs()\n",
        "        if len(rhs) == 1 and isinstance(rhs[0], str):\n",
        "            terminal_map[rhs[0]].append(rule.lhs())\n",
        "\n",
        "    # check current rule, alter it to meet CNF criteria, then add it to new CNF grammar L2\n",
        "    for rule in grammar.productions():\n",
        "        lhs = rule.lhs()\n",
        "        rhs = rule.rhs()\n",
        "\n",
        "        # includes unit productions\n",
        "        if len(rhs) == 1 and isinstance(rhs[0], Nonterminal):\n",
        "            L2.append(Production(lhs, rhs))\n",
        "            continue\n",
        "\n",
        "        # Replace terminals with existing  NT categories (if part of a mixed rule)\n",
        "        updated_rhs = []\n",
        "        for sym in rhs:\n",
        "            if isinstance(sym, str): # if dealing with T symbol\n",
        "                if len(rhs) == 1:\n",
        "                    updated_rhs.append(sym)  # keep terminals in rules like A → 'a'\n",
        "                else: # mixed rule\n",
        "                    if sym in terminal_map:\n",
        "                        updated_rhs.append(terminal_map[sym][0])  # use the first corresponding NT category found\n",
        "                    else:\n",
        "                        # Optional: fallback to dummy nonterminal if terminal has no existing category\n",
        "                        dummy_nt = Nonterminal(f'T_{sym}')\n",
        "                        updated_rhs.append(dummy_nt)\n",
        "                        L2.append(Production(dummy_nt, [sym]))\n",
        "            else:\n",
        "                updated_rhs.append(sym)\n",
        "\n",
        "        # Binarize rules\n",
        "        if len(updated_rhs) <= 2:\n",
        "            L2.append(Production(lhs, updated_rhs))\n",
        "        else:\n",
        "            current_lhs = lhs\n",
        "            for i in range(len(updated_rhs) - 2):\n",
        "                dummy_nt = Nonterminal(f'{padding}{rule_count}')\n",
        "                rule_count += 1\n",
        "                L2.append(Production(current_lhs, [updated_rhs[i], dummy_nt]))\n",
        "                current_lhs = dummy_nt\n",
        "            L2.append(Production(current_lhs, updated_rhs[-2:]))\n",
        "\n",
        "    return L2"
      ],
      "metadata": {
        "id": "uoNYVUYf4Drp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L1 = CFG.fromstring(L1_grammar)\n",
        "L2_grammar = convert_to_CNF(L1)\n",
        "L2 = CFG(Nonterminal('S'), L2_grammar)"
      ],
      "metadata": {
        "id": "yKtZN4nClONd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for rule in L2.productions():\n",
        "    print(rule)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5I5TMIbl0PZ",
        "outputId": "8ee738f4-6571-45f3-b49d-6e0e980f9ebf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S -> NP VP\n",
            "S -> Aux X1\n",
            "X1 -> NP VP\n",
            "NP -> Pronoun\n",
            "NP -> Proper-Noun\n",
            "NP -> Det Nominal\n",
            "Nominal -> Noun\n",
            "Nominal -> Nominal Noun\n",
            "Nominal -> Nominal PP\n",
            "VP -> Verb\n",
            "VP -> Verb NP\n",
            "VP -> Verb PP\n",
            "VP -> Verb X2\n",
            "X2 -> NP PP\n",
            "PP -> Preposition NP\n",
            "Aux -> 'will'\n",
            "Aux -> 'can'\n",
            "Aux -> 'should'\n",
            "Verb -> 'book'\n",
            "Verb -> 'include'\n",
            "Verb -> 'prefer'\n",
            "Pronoun -> 'I'\n",
            "Pronoun -> 'she'\n",
            "Pronoun -> 'me'\n",
            "Proper-Noun -> 'United'\n",
            "Proper-Noun -> 'Houston'\n",
            "Nominal -> 'book'\n",
            "Nominal -> 'include'\n",
            "Nominal -> 'prefer'\n",
            "Noun -> 'book'\n",
            "Noun -> 'trip'\n",
            "Noun -> 'hotel'\n",
            "Det -> 'the'\n",
            "Det -> 'a'\n",
            "Preposition -> 'in'\n",
            "Preposition -> 'on'\n",
            "Preposition -> 'of'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18.2 + 18.3\n",
        " Implement the CKY algorithm and test it with your converted L1 grammar. +\n",
        "Rewrite the CKY algorithm given in Fig. 18.12 on page 14 so that it can accept grammars that contain unit productions."
      ],
      "metadata": {
        "id": "pfOCS3sA4ErO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*For 18.3, inside of an additional \"reformat\" method, I implemented a dict that contains all corresponding Nonterminals for a given terminal. For each terminal, I found all possible unit production conversions (e.g. A -> B -> C). When initializing the CKY DP table with terminals, I added new leaves corresponding to each terminals' additional unit production conversions (e.g. (A->B) and (A->C)). under the corresponding terminal so that the algorithm would include all possible valid grammatical structures that could be derived from those unit productions.*"
      ],
      "metadata": {
        "id": "eDCR5dU4jTmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reformat(grammar):\n",
        "    '''extra method that allows CKY parser to process unit productions,\n",
        "    convert grammar into a reversed dictionary for faster lookup,\n",
        "    generate additional dictionary of just 1-to-1 maps for processing unit productions'''\n",
        "    flipped = {}\n",
        "    simple_map = {}\n",
        "    single_mappings = {}\n",
        "\n",
        "    # map each rhs to its corresponding lhs (reversed grammar mapping for easy lookup during parsing)\n",
        "    for rule in grammar.productions():\n",
        "        rhs = rule.rhs()\n",
        "        lhs = rule.lhs()\n",
        "        if rhs not in flipped:\n",
        "            flipped[rhs] = set()\n",
        "            flipped[rhs].add(lhs)\n",
        "\n",
        "        #initialize dict to store all possible 1:1 conversions\n",
        "        if len(rhs) == 1:\n",
        "            if nltk.grammar.is_terminal(rhs):\n",
        "                if rhs not in simple_map:\n",
        "                    simple_map[rhs] = (lhs,)\n",
        "                elif rhs in simple_map:\n",
        "                    simple_map[rhs] += (lhs,)\n",
        "\n",
        "    # initialize mapping of unit productions where all possible unit conversions for each terminal are explored and added\n",
        "    for child in simple_map:\n",
        "        if nltk.grammar.is_terminal(child[0]):\n",
        "            parents = simple_map[child]\n",
        "            single_mappings[child] = [parents[0]]\n",
        "            for parent in parents:\n",
        "                curr_parent = (parent,)\n",
        "                while curr_parent in simple_map:\n",
        "                   single_mappings[child].append(simple_map[curr_parent])\n",
        "                   curr_parent = simple_map[curr_parent]\n",
        "\n",
        "    return flipped, single_mappings\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sFwJyjOch-o7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CKY(words, grammar):\n",
        "    '''Bottom-up diagonal CKY parser for CNF grammar, using nltk.Tree'''\n",
        "    # pre-process given grammar (faster lookup, also allows CKY to handle unit productions)\n",
        "    grammar, u_p = reformat(grammar)\n",
        "    n = len(words)\n",
        "\n",
        "    # setup n * n DP matrix\n",
        "    table = [[[] for _ in range(n+1)] for _ in range(n)]\n",
        "\n",
        "    # initialize table with terminals (A -> 'a')\n",
        "    for i, word in enumerate(words):\n",
        "        seen = set() # keep track of trees, eliminates duplicates\n",
        "        if (word,) in grammar:\n",
        "            for lhs in grammar[(word,)]:\n",
        "                lhs_label = lhs[0] if isinstance(lhs, tuple) else lhs\n",
        "                # store table containing 'parent' rule (lhs) and corresponding terminal (word)\n",
        "                tree = Tree(lhs_label, [word])\n",
        "                # key for keeping tack of trees being stored (memoization)\n",
        "                key = (tree.label(), tuple(tree))\n",
        "                if key not in seen:\n",
        "                    table[i][i+1].append(tree)\n",
        "                    seen.add(key)\n",
        "            # fill cell of each terminal with all possible unit productions that can be extracted from its corresponding lhs(s)\n",
        "            for lhs in u_p[(word,)]:\n",
        "                lhs_label = lhs[0] if isinstance(lhs, tuple) else lhs\n",
        "                tree = Tree(lhs_label, [word])\n",
        "                key = (tree.label(), tuple(tree))\n",
        "                if key not in seen:\n",
        "                    table[i][i+1].append(tree)\n",
        "                    seen.add(key)\n",
        "\n",
        "    # Fill table with binary rules (A -> B C)\n",
        "    # check all possible grammatical combinations across span num of trees\n",
        "    for span in range(2, n+1):          # span length\n",
        "        for i in range(n - span + 1):    # span start\n",
        "            j = i + span                 # span end\n",
        "            seen = set()\n",
        "            for k in range(i+1, j):      # split point\n",
        "                for left_tree in table[i][k]:\n",
        "                    for right_tree in table[k][j]:\n",
        "                        rhs_label = right_tree.label()\n",
        "                        if isinstance(rhs_label, tuple):\n",
        "                            rhs_label = rhs_label[0]\n",
        "                        rhs = (left_tree.label(), rhs_label)\n",
        "                        # see if combination of rules from left tree and right tree is valid (exists in grammar)\n",
        "                        # if it is, add it to the DP table at i,j\n",
        "                        if rhs in grammar:\n",
        "                            for lhs in grammar[rhs]:\n",
        "                                tree = Tree(lhs, [left_tree, right_tree])\n",
        "                                key = (tree.label(), tuple(child if isinstance(child, str) else (child.label(),) for child in tree))\n",
        "                                if key not in seen:\n",
        "                                    table[i][j].append(tree)\n",
        "                                    seen.add(key)\n",
        "    # return last element of table (contains the possible parse tree(s) for entire sentence)\n",
        "    return table[0][n]\n"
      ],
      "metadata": {
        "id": "pP1CLRgA0ibB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test sentence\n",
        "words = ['I', 'book', 'the', 'trip', 'in', 'Houston']"
      ],
      "metadata": {
        "id": "EQlDQwr06h8-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g, p = reformat(L2)\n",
        "parses=CKY(words, L2)"
      ],
      "metadata": {
        "id": "5Md-55ItjVFx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for parse in parses:\n",
        "    display(parse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "p_Z2l3fDoHXd",
        "outputId": "10417631-af68-443c-8511-24b1eeaa5523"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tree(S, [Tree(NP, ['I']), Tree(VP, [Tree(Verb, ['book']), Tree(NP, [Tree(Det, ['the']), Tree(Nominal, [Tree(Nominal, ['trip']), Tree(PP, [Tree(Preposition, ['in']), Tree(NP, ['Houston'])])])])])])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"312px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,368.0,312.0\" width=\"368px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"8.69565%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">I</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"4.34783%\" y1=\"20px\" y2=\"48px\" /><svg width=\"91.3043%\" x=\"8.69565%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"14.2857%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Verb</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">book</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.14286%\" y1=\"20px\" y2=\"48px\" /><svg width=\"85.7143%\" x=\"14.2857%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"13.8889%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Det</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.94444%\" y1=\"20px\" y2=\"48px\" /><svg width=\"86.1111%\" x=\"13.8889%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Nominal</text></svg><svg width=\"29.0323%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Nominal</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">trip</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.5161%\" y1=\"20px\" y2=\"48px\" /><svg width=\"70.9677%\" x=\"29.0323%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PP</text></svg><svg width=\"59.0909%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Preposition</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"29.5455%\" y1=\"20px\" y2=\"48px\" /><svg width=\"40.9091%\" x=\"59.0909%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Houston</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.5455%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.5161%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.9444%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.1429%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"54.3478%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18.4\n",
        "Discuss how to augment a parser to deal with input that may be incorrect, for example, containing spelling errors or mistakes arising from automatic speech recognition."
      ],
      "metadata": {
        "id": "7yePcOoM4KfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*I think that a transformer model (or other neural networks capable of handling long sequences) would be highly beneficial for this specific task. Transformers can track long-range dependencies (important for understanding context in sentences), and they can be trained to process a high volume of linguistic data. In this situation, you could implement a transformer to identify if the sentence provided for parsing makes syntactic sense. After training the transformer on lots of grammatically and syntactically valid sentences, it could be trained to process a given sentence and provide a 'corrected' version of the sentence that makes more sense contextually. This will make it more likely that the parser will work correctly given noisy input.*"
      ],
      "metadata": {
        "id": "7GS3utPjROFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18.5\n",
        "Implement the PARSEVAL metrics described in Section 18.8. Next, use a\n",
        "parser and a treebank, compare your metrics against a standard implementation. Analyze the errors in your approach."
      ],
      "metadata": {
        "id": "BuNMZWMZPW1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The textbook was not very clear on how to implement a PARSEVAL algorithm besides providing definitions for the metrics themselves. I decided to simply implement the standard PARSEVAL evaluation tool \"evalb\" which unfortunately is rather outdated. Its documentation is not accurate, and both only inputs/outputs txt files, which is very difficult to do under the constraints of Colab and when trying to work with nltk Tree objects, not txt files or corpora.*\n",
        "\n",
        "*I tested my CKY parser using the nltk treebank. I extracted grammars directly from these trees and plugged them into my alg alongside unlabelled sentences corresponding to the given grammar. My parser did not always give me a parse, and I am not completely sure why this is. However, when it did give me a parse and I was able to run the parse eval tool on it succesfully, I would get 100% accuracy.*\n",
        "\n",
        "*My prediction for why my algorithm doesn't work all the time is that I only implement unit productions for the terminals and do not check them at every cell. For instance, if given a case where A and B have no unit productions, but C -> AB has a unit production C -> D, then D will be excluded and the parse could fail. I didn't implement this because finding all possible unit productions for any given NT/T is computationally expensive in its own right (this would be a recursive process unless some DP process that I am unaware of exists) and it would make the entire parser take even longer to run.*"
      ],
      "metadata": {
        "id": "eiF-WEriR9pV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "run parser on treebank data:"
      ],
      "metadata": {
        "id": "fHbN5WT6VnT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import treebank\n",
        "nltk.download('treebank')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI7jkoAx-LQ-",
        "outputId": "bece9d9a-58cb-4b83-9eff-0156b2b2ba71"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = 0\n",
        "y = x+1\n",
        "trees = treebank.parsed_sents()[x:y]\n",
        "productions = []\n",
        "for tree in trees:\n",
        "    tree.chomsky_normal_form(horzMarkov=2)\n",
        "    productions += tree.productions()\n",
        "\n",
        "S = Nonterminal('S')\n",
        "cfg = CFG(S, productions)\n",
        "sentences = treebank.sents()[x:y]"
      ],
      "metadata": {
        "id": "qdt8xxjP5WbM"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "for i, sentence in enumerate(sentences):\n",
        "    parses = CKY(sentence, cfg)\n",
        "    predictions.append(parses)\n"
      ],
      "metadata": {
        "id": "hcAm8Yv6Eh_g"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARSEVAL standard approach:"
      ],
      "metadata": {
        "id": "OWQITP1xEUGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PYEVALB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4eJaqQ0EVZG",
        "outputId": "8b9e8c27-eb82-4c4e-aab4-43564bd927e7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PYEVALB in /usr/local/lib/python3.11/dist-packages (0.1.3)\n",
            "Requirement already satisfied: pytablewriter>=0.10.2 in /usr/local/lib/python3.11/dist-packages (from PYEVALB) (1.2.1)\n",
            "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.11/dist-packages (from pytablewriter>=0.10.2->PYEVALB) (75.2.0)\n",
            "Requirement already satisfied: DataProperty<2,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from pytablewriter>=0.10.2->PYEVALB) (1.1.0)\n",
            "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pytablewriter>=0.10.2->PYEVALB) (1.1.4)\n",
            "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from pytablewriter>=0.10.2->PYEVALB) (3.2.3)\n",
            "Requirement already satisfied: tabledata<2,>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from pytablewriter>=0.10.2->PYEVALB) (1.3.4)\n",
            "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from pytablewriter>=0.10.2->PYEVALB) (0.1.7)\n",
            "Requirement already satisfied: typepy<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter>=0.10.2->PYEVALB) (1.3.4)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.11/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter>=0.10.2->PYEVALB) (5.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter>=0.10.2->PYEVALB) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.11/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter>=0.10.2->PYEVALB) (2025.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter>=0.10.2->PYEVALB) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.8.0->typepy[datetime]<2,>=1.3.2->pytablewriter>=0.10.2->PYEVALB) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PYEVALB import scorer\n",
        "from PYEVALB import parser\n",
        "from PYEVALB import tree as pyevalb_tree_module\n",
        "import tempfile"
      ],
      "metadata": {
        "id": "5Zu97LafEctN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_bracket_strings(gold_bracket, pred_bracket):\n",
        "    '''since pyevalb only works with txt files, I had to convert my predicted tree and gold standard trees into temporary txt files\n",
        "    and then run the evalb alg on them'''\n",
        "    result_path = tempfile.NamedTemporaryFile(delete=False).name\n",
        "    s = scorer.Scorer()\n",
        "\n",
        "    with tempfile.NamedTemporaryFile('w+', delete=False) as gold_file, \\\n",
        "         tempfile.NamedTemporaryFile('w+', delete=False) as pred_file, \\\n",
        "         tempfile.NamedTemporaryFile('w+', delete=False) as result_file:\n",
        "        gold_file.write(gold_bracket)\n",
        "        gold_file.flush()\n",
        "        pred_file.write(pred_bracket)\n",
        "        pred_file.flush()\n",
        "\n",
        "        results = s.evalb(gold_file.name, pred_file.name, result_file.name)\n",
        "        evalb_output = result_file.read()\n",
        "    # outputs txt analysis of predicted vs gold standard parse trees computed using PARSEVAL metrics (recall, precision, crossing, accuracy)\n",
        "    return evalb_output"
      ],
      "metadata": {
        "id": "NYZUtcu0j2P4"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run PARSEVAL alg to compare predicted tree(s) by CKY parser vs. corresponding gold standard tree from treebank\n",
        "results = []\n",
        "for gold_tree in trees:\n",
        "    gold_tree_reformatted = gold_tree.pformat(margin=1e10)\n",
        "    for pred in predictions:\n",
        "        if len(pred) == 0: continue\n",
        "        for pred_tree in pred:\n",
        "            pred_tree_reformatted = pred_tree.pformat(margin=1e10)\n",
        "            if len(gold_tree_reformatted) == len(pred_tree_reformatted):\n",
        "                r = score_bracket_strings(gold_tree_reformatted, pred_tree_reformatted)\n",
        "                results.append(r)\n"
      ],
      "metadata": {
        "id": "aH6_C1norQqZ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hUIOU2rw3bU",
        "outputId": "734aef5d-79d8-4f78-d59b-45b8382284f6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['# Score Result\\n| A | B | C | D | E | F | G | H | I | J | K | L |\\n|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|--:|\\n|  0| 18|  0|  1|  1| 17| 17| 17|  0| 18| 18|  1|\\n\\n=================================================================================================================================================\\nNumber of sentence:\\t1.00\\nNumber of Error sentence:\\t0.00\\nNumber of Skip  sentence:\\t0.00\\nNumber of Valid sentence:\\t1.00\\nBracketing Recall:\\t100.00\\nBracketing Precision:\\t100.00\\nBracketing FMeasure:\\t100.00\\nComplete match:\\t100.00\\nAverage crossing:\\t0.00\\nNo crossing:\\t100.00\\nTagging accuracy:\\t100.00']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}